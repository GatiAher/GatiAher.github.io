<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.68.3" />
  
  
  <meta property=”og:title” content="Gati Aher's Portfolio" />
  <meta property=”og:image” content=”/img/cover_page.png” />
  <meta property="og:description"
    content="I am currently studying computer science at Olin College of Engineering, class of 2023.
  I use this blog to document my software projects, share my data analysis adventures, and reflect upon design processes." />
  <meta property="og:url" content="https://gatiaher.github.io/" />
  
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  
  <link rel="stylesheet" href="/css/style.css">
  
  
  <title>Handwriting Detection With Faster R-CNN &#43; Experiments</title>
  

<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png">
  <link rel="manifest" href="/favicon_io/site.webmanifest">
</head>

<body>
    <div class="site-header">
    <nav id="nav" class="container">
        <div class=bar>
            <a class="navbar-brand brand" href="/">
                Gati Aher
            </a>
            <div class="links">
                
                
                
                <a class="nav-link" href="/projects"><i data-feather="archive"></i> All Projects</a>
                
                
                
                <a class="nav-link" href="/tags"><i data-feather="tag"></i> Tags</a>
                
                
                
                <a class="nav-link" href="/about"><i data-feather="at-sign"></i> Contact</a>
                
            </div>
        </div>
    </nav>
</div>

    <main id="main" class="container">
        

<div>
    

    <div class="category">
    
    
    
    
    <a class="categories selective-underline" href="/categories/deep-learning">Deep Learning</a>
    
    
    
</div>
    <div class="cover-image-container">
        <div class="cover-image-container">
    
    
    
    
    <img src="http://GatiAher.github.io/projects/handwriting-detection-with-faster-r-cnn-plus-experiments/cover_hu491f3a7f448df3047e7c085d7496a940_42636_148a497c77621593d0508ea83d9c08f4.png" />
    
    
    
</div>
        <div class="cover-image-text-block">
            <h1>Handwriting Detection With Faster R-CNN &#43; Experiments</h1>
        </div>
    </div>


    <div class="tags">
    
    
    
    
    <a class="tags selective-underline" href="/tags/indico-data-solutions-intern-rd-&#43;-ml-engineering">Indico Data Solutions (Intern: R&amp;D &#43; ML Engineering)</a>
    
    
    
</div>
    <div class="meta">
    By Gati Aher &#183
    

<time class="date" datetime="2021-08-20">Aug 20, 2021</time> &#183
    37 min read
</div>

    

</div>

<br/>

<article>

    <p><a href="https://indico.io/">Indico Data Solutions</a> provides services to extract information from scanned pdfs. Since their existing OCR + NLP pipeline did not extract handwriting, one of my internship projects involved creating a robust solution to detect and classify handwriting using a deep learning computer vision model. I started by fine-tuning upon the <a href="https://arxiv.org/abs/1506.01497">Faster R-CNN model</a> from the <a href="https://github.com/facebookresearch/detectron2">Detectron-v2 framework</a>. Then I tried to improve upon the baseline performance with (i) different pre-training tasks, (ii) multi-label formulation, (iii) strategies to improve small object detection, and (iv) different label sets and datasets. This report documents my methods and finishes with a class confusion analysis and retrospective.</p>
<ul>
<li><a href="#1-introduction">1 INTRODUCTION</a>
<ul>
<li><a href="#11-datasets">1.1 Datasets</a></li>
<li><a href="#12-tough-to-beat-baseline-model">1.2 Tough-to-Beat Baseline Model</a></li>
</ul>
</li>
<li><a href="#2-background-information">2 BACKGROUND INFORMATION</a>
<ul>
<li><a href="#21-overview-of-detectron-v2s-faster-r-cnn-with-fpn">2.1 Overview of Detectron-v2&rsquo;s Faster R-CNN with FPN</a></li>
<li><a href="#22-overview-of-fine-tuning">2.2 Overview of Fine-Tuning</a></li>
</ul>
</li>
<li><a href="#3-fine-tune-on-models-pre-trained-on-documents">3 FINE-TUNE ON MODELS PRE-TRAINED ON DOCUMENTS</a>
<ul>
<li><a href="#31-check-for-porting-discrepancies">3.1 Check for Porting Discrepancies</a></li>
<li><a href="#32-results">3.2 Results</a></li>
</ul>
</li>
<li><a href="#4-expand-the-label-set">4 EXPAND THE LABEL SET</a>
<ul>
<li><a href="#41-results">4.1 Results</a></li>
<li><a href="#42-analysis-of-class-confusion-via-visual-inspection">4.2 Analysis of Class Confusion Via Visual Inspection</a></li>
<li><a href="#43-considerations-on-an-alternative-formulation">4.3 Considerations on an Alternative Formulation</a></li>
</ul>
</li>
<li><a href="#5-multi-label-object-detection">5 MULTI-LABEL OBJECT DETECTION</a>
<ul>
<li><a href="#51-detectron-v2-modifications">5.1 Detectron-v2 Modifications</a></li>
<li><a href="#52-new-label-sets">5.2 New Label Sets</a></li>
<li><a href="#53-results">5.3 Results</a></li>
</ul>
</li>
<li><a href="#6-better-small-object-detection">6 BETTER SMALL OBJECT DETECTION</a>
<ul>
<li><a href="#61-visualize-feature-map">6.1 Visualize Feature Map</a></li>
<li><a href="#62-increase-image-resoluton">6.2 Increase Image Resoluton</a></li>
<li><a href="#63-add-smaller-anchor-size">6.3 Add Smaller Anchor Size</a></li>
<li><a href="#64-results">6.4 Results</a></li>
</ul>
</li>
<li><a href="#7-test-on-out-of-distribution-data">7 TEST ON OUT-OF-DISTRIBUTION DATA</a>
<ul>
<li><a href="#71-results">7.1 Results</a></li>
</ul>
</li>
<li><a href="#8-train-final-all_document-model">8 TRAIN FINAL ALL_DOCUMENT MODEL</a>
<ul>
<li><a href="#81-results">8.1 Results</a></li>
<li><a href="#82-train-validation-loss-curve">8.2 Train-Validation Loss Curve</a></li>
</ul>
</li>
<li><a href="#9-qualitative-and-quantitative-analysis-of-object-detection-class-confusion-false-positives-and-false-negatives">9 QUALITATIVE AND QUANTITATIVE ANALYSIS OF OBJECT DETECTION CLASS CONFUSION, FALSE POSITIVES, AND FALSE NEGATIVES</a>
<ul>
<li><a href="#91-performance-on-lease-png-label-set-v1">9.1 Performance on Lease-PNG label set v1</a></li>
<li><a href="#92-performance-on-lease-png-label-set-v2">9.2 Performance on Lease-PNG label set v2</a></li>
<li><a href="#93-performance-on-all_docs-label-set-v5-final-model">9.3 Performance on All_Docs label set v5 (Final Model)</a></li>
</ul>
</li>
<li><a href="#10-future-work">10 FUTURE WORK</a>
<ul>
<li><a href="#101-analysis-of-prediction-bounding-box-overlap">10.1 Analysis of Prediction Bounding Box Overlap</a></li>
<li><a href="#102-reduce-false-positives-with-explicit-labeling">10.2 Reduce False Positives with Explicit Labeling</a></li>
<li><a href="#103-reduce-inference-time-and-memory-usage">10.3 Reduce Inference Time and Memory Usage</a></li>
<li><a href="#104-find-out-why-all_docs-model-performed-poorly-on-checkbox-detection">10.4 Find Out Why All_Docs Model Performed Poorly on Checkbox Detection</a></li>
</ul>
</li>
<li><a href="#11-appendix-faster-rcnn-implementation-details">11 APPENDIX: FASTER-RCNN IMPLEMENTATION DETAILS</a>
<ul>
<li><a href="#datamapper">DataMapper</a></li>
<li><a href="#backbone-network">Backbone Network</a>
<ul>
<li><a href="#details-on-feature-pyramid-network">Details on Feature Pyramid Network</a></li>
</ul>
</li>
<li><a href="#region-proposal-network-rpn">Region Proposal Network (RPN)</a>
<ul>
<li><a href="#rpn-head">RPN Head</a>
<ul>
<li><a href="#map-ground-truths-to-feature-maps-with-cell-anchors-during-training">Map Ground Truths to Feature Maps With Cell Anchors (during training)</a></li>
<li><a href="#loss-function-during-training">Loss Function (during training)</a></li>
<li><a href="#box-proposal-selection">Box Proposal Selection</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#roi-head">ROI Head</a>
<ul>
<li><a href="#re-sampling-and-matching-during-training">Re-Sampling and Matching (during training)</a></li>
<li><a href="#cropping">Cropping</a></li>
<li><a href="#box-head">Box Head</a>
<ul>
<li><a href="#loss-calculation">Loss Calculation</a></li>
<li><a href="#inference">Inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="1-introduction">1 INTRODUCTION</h1>
<p>For my internship this summer I was lucky enough to spend my time at Indico Data Solutions as a Research &amp; Development machine learning intern. One of my projects focused on fine-tuning and modifying a pre-trained computer vision object-detection model to detect and classify handwritten marks on documents.</p>
<p>One service Indico provides is extracting information from unstructured documents. This is done by first using OCR to grab printed text from pdfs, and then running a fine-tuned natural language processing model to perform Named Entity Recognition (NER). This existing process does not capture handwritten information that clients might find useful. Utilizing an object-detection model is one way of capturing the missed information.</p>
<p>In this article, I will give an overview of the Faster-R-CNN architecture, discuss experiments and results, and finish by covering future directions to research. For this project, I used the Faster R-CNN model from Detectron-v2 framework, and experimented with (1) different pre-training tasks, (2) multi-label formulation, (3) strategies to improve small object detection, (4) expanded label sets and datasets.&rdquo;</p>
<h2 id="11-datasets">1.1 Datasets</h2>
<p>Indico’s labeling team annotated a set of public government lease documents with bounding box locations and multi-labels. Each annotated bounding box could have one or more of the following labels: <code>“Signature”, “Redaction”, “Unfilled Checkbox”, “Filled Checkbox”, “Strikethrough”, “Initials”, “Date”, “Address”, “Name”, “Title”, ”Other Text”, “Stamp”, “Printed”, ”Correction”, “Empty Signature Zone”, “Empty”</code>.</p>
<p>For the baseline experiment, we used a reduced label set (v1) where we kept visually obvious labels with a large number of counts, mapped visually similar labels to the same label, excluded labels with fewer than ~200 counts, and performed aggregate mapping and exclusion.</p>
<p>The original classes <code>“Signature”, “Redaction”, “Unfilled Checkbox”, “Filled Checkbox”</code> were kept, the classes <code>“Initials”, “Date”, “Address”, “Name”, ”Other Text”</code>, were mapped to <code>“Handwriting”</code>, the rare instances of <code>&quot;Stamp&quot;, &quot;Empty&quot;, &quot;Empty Signature Zone&quot;, &quot;Title&quot;, &quot;Correction&quot;, &quot;Printed&quot;, &quot;Strikethrough&quot;, “Strikethrough4”</code> were ignored. The multi-label <code>&quot;Handwriting-Signature&quot;</code> was mapped to <code>&quot;Signature&quot;</code>, and the multi-labels <code>&quot;Filled Checkbox-Unfilled Checkbox&quot;</code> and <code>&quot;Filled Checkbox-Redaction&quot;</code> were ignored.</p>
<p>All the datasets used in this experiment were public government lease documents.</p>
<p><strong>Lease PNGs Dataset (2339 documents)</strong>: Used for experiments. Split 80:10:10 (train, test, and dev).</p>
<p>Label Counts in Whole Dataset</p>
<pre><code>all_counts: {'Signature': 2333, 'Other Text': 1095, 'Redaction': 3102, 'Date': 1307, 'Initials': 3433, 'Stamp': 401, 'Address': 365, 'Strikethrough': 230, 'Name': 1024, 'Filled Checkbox': 901, 'Unfilled Checkbox': 538, 'Empty Signature Zone': 65, 'Printed': 47, 'Title': 65, 'Correction': 4, 'Empty': 3, 'Strikethrough4': 4}
</code></pre><p>Multi-Label Counts in Whole Dataset</p>
<pre><code>joined_counts: {'Signature': 2330, 'Other Text': 1041, 'Redaction': 3101, 'Date': 1093, 'Initials': 3403, 'Initials-Stamp': 24, 'Address': 354, 'Strikethrough': 215, 'Name': 891, 'Filled Checkbox': 898, 'Unfilled Checkbox': 532, 'Date-Stamp': 202, 'Empty Signature Zone': 65, 'Name-Stamp': 114, 'Other Text-Stamp': 43, 'Name-Strikethrough': 2, 'Name-Other Text-Stamp': 5, 'Date-Initials': 5, 'Stamp': 7, 'Date-Other Text-Signature-Stamp': 1, 'Stamp-Strikethrough': 1, 'Printed-Title': 25, 'Address-Printed': 9, 'Name-Printed': 11, 'Title': 38, 'Correction': 2, 'Correction-Other Text': 2, 'Stamp-Title': 2, 'Address-Empty': 1, 'Empty-Initials': 1, 'Date-Empty': 1, 'Date-Printed': 2, 'Signature-Stamp': 1, 'Filled Checkbox-Strikethrough': 1, 'Strikethrough-Unfilled Checkbox': 5, 'Other Text-Strikethrough': 3, 'Name-Signature': 1, 'Date-Strikethrough': 3, 'Address-Stamp': 1, 'Filled Checkbox-Redaction': 1, 'Filled Checkbox-Unfilled Checkbox': 1, 'Strikethrough4': 4}
</code></pre><p><strong>FCC PNGs Dataset (2087 documents)</strong>: Used as an out-of-distribution testing dataset for models trained on the Lease-png dataset.</p>
<p><strong>All Documents Dataset (5871 documents)</strong>: A bigger, more varied dataset. Used to train final production model. Split 80:10:10 (train, test, and dev).</p>
<p>The full set of documents was comprised of the following data sets:</p>
<ul>
<li>Lease PNGs</li>
<li>FCC PNGs</li>
<li>TAPAS PNGs</li>
<li>UCSF batch 1 and 2</li>
<li>UFO PNGs</li>
</ul>
<h2 id="12-tough-to-beat-baseline-model">1.2 Tough-to-Beat Baseline Model</h2>
<p>The Indico object-detection repo supported fine-tuning from the weights of two models:</p>
<ol>
<li>baseline-model: <code>faster_rcnn_X_101_32x8d_FPN_3x_139173657_model_final_68b088</code></li>
<li>smaller-baseline-model: <code>faster_rcnn_R_50_FPN_3x_137849458_model_final_280758</code></li>
</ol>
<p>These baseline models have a Faster R-CNN architecture with a backbone net pre-trained on ImageNet classification, and RPN and ROIHead pre-trained on COCO (Common Objects in Context) object-detection tasks</p>
<p><strong>Baseline Results</strong></p>
<p>The object detection evaluation metrics use Intersection-over-Union (IoU) to pair prediction bounding boxes with ground truth bounding boxes.</p>
<figure>
    <img src="img/definition_iou.png"
         alt="Definition of Object-Detection: Intersection-over-Union (IoU)"/> <figcaption>
            <p>Definition of Object-Detection: Intersection-over-Union (IoU)</p>
        </figcaption>
</figure>

<p>The models that assigned a single label to each bounding box were evaluated with <a href="https://cocodataset.org/#detection-eval">COCOEvaluator’s metrics</a> for average precision, average recall, and per class average precision.</p>
<figure>
    <img src="img/definition_of_precision_recall_iou.png"
         alt="Definition of Object-Detection: Precision, Recall, IoU"/> <figcaption>
            <p>Definition of Object-Detection: Precision, Recall, IoU</p>
        </figcaption>
</figure>

<p>Average Precision (AP) and Average Recall (AR) is measured out of 100 points t various thresholds. AP Numbers in the 40s are good. We care most about AP per class and AP50. For metrics tables in this report, cells are colored linearly with respect to the min and max values of the given table.</p>
<p>Fine-tuning the baseline and smaller-baseline models to the handwriting detection task using label set v1 and Lease PNG dataset gave the following results:</p>
<figure>
    <img src="img/eval_AP_baseline.png"
         alt="Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_baseline.png"
         alt="Average Recall (COCOEvaluator)"/> <figcaption>
            <p>Average Recall (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_baseline.png"
         alt="Per Class Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/pred_model_prediction.png"
         alt="Lease PNG document image (LID07240-Lease-a_Z-1.png) with fine-tuned baseline model predictions."/> <figcaption>
            <p>Lease PNG document image (LID07240-Lease-a_Z-1.png) with fine-tuned baseline model predictions.</p>
        </figcaption>
</figure>

<p><em>COLOR KEY</em></p>
<ul>
<li>Predictions have tinted fill, ground truth label has no fill</li>
<li>pink: Filled Checkbox</li>
<li>blue: Unfilled Checkbox</li>
<li>teal: Handwriting</li>
<li>red: Signature</li>
<li>yellow: Redaction</li>
<li>orange: Date (in label set v2 and v5)</li>
</ul>
<p>The baseline model performs quite well. The following analysis investigate error modes and attempts to beat the baseline model.</p>
<h1 id="2-background-information">2 BACKGROUND INFORMATION</h1>
<h2 id="21-overview-of-detectron-v2s-faster-r-cnn-with-fpn">2.1 Overview of Detectron-v2&rsquo;s Faster R-CNN with FPN</h2>
<p>Detectron2 is Facebook AI Research’s library for object detection and segmentation algorithms. In this project, I used the Faster R-CNN with Feature Pyramid Network (FPN) architecture, which is the basic multi-scale bounding box detector with high accuracy in detecting tiny to large objects. It has three main parts: Backbone Network, Region Proposal Network, and ROI Head (Box Head). Becoming familiar with the Detectron-v2 framework and the Faster R-CNN with FPN architecture was a major part of my learning experience this summer. See the <a href="#11-appendix-faster-rcnn-implementation-details">Appendix</a> for my notes and observations.</p>
<p><strong>Useful Resources</strong></p>
<ul>
<li><a href="https://github.com/facebookresearch/detectron2">Detectron-v2 GitHub Repo</a>
<ul>
<li><a href="https://github.com/facebookresearch/detectron2/blob/5e2a1ecccd228227c5a605c0a98d58e1b2db3640/detectron2/config/defaults.py">Default Configs</a></li>
<li><a href="https://github.com/facebookresearch/detectron2/blob/5e2a1ecccd228227c5a605c0a98d58e1b2db3640/configs/Base-RCNN-FPN.yaml#L9-L11">Default Base-RCNN-FPN Configs</a></li>
</ul>
</li>
<li>Papers:
<ul>
<li><a href="https://arxiv.org/abs/1512.03385">[1512.03385] Deep Residual Learning for Image Recognition</a></li>
<li><a href="https://arxiv.org/abs/1504.08083">[1504.08083] Fast R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1506.01497">[1506.01497] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></li>
</ul>
</li>
<li><a href="https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd">Useful 5-part explanation of repo structure and Faster R-CNN with FPN implementation By Hiroto Honda</a></li>
</ul>
<figure>
    <img src="img/Faster_R-CNN.png"
         alt="Detailed architecture of Base-RCNN-FPN. Blue labels represent detectron-v2 class names. (Source: Digging into Detectron 2 — part 1 | by Hiroto Honda)"/> <figcaption>
            <p>Detailed architecture of Base-RCNN-FPN. Blue labels represent detectron-v2 class names. (Source: <a href="https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd">Digging into Detectron 2 — part 1 | by Hiroto Honda</a>)</p>
        </figcaption>
</figure>

<h2 id="22-overview-of-fine-tuning">2.2 Overview of Fine-Tuning</h2>
<p>Fine-tuning is a technique that lets you take the weights of a trained neural network (source model) and use it as initialization for a new model (target model) that you want to train on data from the same domain. This technique lets you achieve good performance with a faster training time and small dataset size.</p>
<p><strong>Useful Resources</strong> - <a href="https://d2l.ai/chapter_computer-vision/fine-tuning.html">Good conceptual understanding of fine-tuning</a></p>
<p>When I fine-tune on a detectron-v2 model, I get this warning message:</p>
<pre><code>Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (20, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
</code></pre><p>These warnings appear because the source model&rsquo;s final layer had shape and weights to project into 80+1 and 80x4 tensors, but the new target model has a different number of classes and therefore has a different final layer projection.</p>
<h1 id="3-fine-tune-on-models-pre-trained-on-documents">3 FINE-TUNE ON MODELS PRE-TRAINED ON DOCUMENTS</h1>
<p>The first experiment involved fine-tuning on Layout Parser Models.</p>
<ul>
<li>Layout Parser Website: <a href="https://layout-parser.github.io/">https://layout-parser.github.io/</a></li>
<li>Layout Parser Repo: <a href="https://github.com/Layout-Parser/layout-parser">https://github.com/Layout-Parser/layout-parser</a></li>
</ul>
<p>Despite being pre-trained on “natural” images, the ImageNet + COCO pre-trained baseline models seem to generalize well to document images. But perhaps models pre-trained on documents would do even better. To test this hypothesis, I could fine-tune on layout-parser models that are pre-trained on document images.</p>
<p>Layout Parser offered the following models:</p>
<ul>
<li>HJDataset &ndash; trained on Japanese documents</li>
<li>Prima &ndash; too small</li>
<li>PubLayNet &ndash; Incompatible large model of type mask rcnn, trained on academic papers
<ul>
<li>LayoutParser gives the following note: “For PubLayNet models, we suggest using <code>mask_rcnn_X_101_32x8d_FPN_3x model</code> as it’s trained on the whole training set, while others are only trained on the validation set (the size is only around 1/50). You could expect a 15% AP improvement using the <code>mask_rcnn_X_101_32x8d_FPN_3x</code> model.”</li>
</ul>
</li>
<li>Newspaper Navigator &ndash; large but trained on Newspapers</li>
<li>TableBank &ndash; large but trained on academic papers</li>
</ul>
<p>None of the models were perfect for the domain use-case of unstructured business documents. Of the available options, the largest Faster-RCNN models of NewspaperNavigator and the TableBanks seemed like the best choices. We decided to compare the following bounding box object detection models to the model baseline and small model baseline:</p>
<ul>
<li>NewspaperNavigator: <code>faster_rcnn_R_50_FPN_3x</code></li>
<li>TableBank: <code>faster_rcnn_R_50_FPN_3x</code></li>
<li>TableBank: <code>faster_rcnn_R_101_FPN_3x</code></li>
</ul>
<p>Porting the models into Indico’s platform for fine-tuning was quite simple since both layout-parser repo and indico’s object-detection repo were both built on top of detectron-v2. I just had to download the model weights and configuration file from the layout parser zoo and make a couple minor code changes.</p>
<h2 id="31-check-for-porting-discrepancies">3.1 Check for Porting Discrepancies</h2>
<p>I wanted to make sure that the LayoutParser platform and indico object-detection platform returned the same outputs given the same input image and model, and that there were no pre- or post-processing discrepancies.</p>
<p>To check for preprocessing discrepancy, I saved the image right before model prediction and checked that the images were identical.</p>
<pre><code>&gt;&gt;&gt; torch.all(loaded_indico[&quot;image&quot;].eq(loaded_layoutparser[&quot;image&quot;]))
tensor(True)
</code></pre><p>Then, to check that predictions matched, I wrote a script to load model weights into indico’s object-detection platform and predict with them directly (without fine-tuning).</p>
<pre><code>path to image: data/fcc_2c3a1ab2-0c2a-4a9e-96d4-1715719c1fde_01.png
Trying out indico...
** _load_base_model cfg.MODEL.WEIGHTS /indicoapi_data/layout_parser_models/TableBank_faster_rcnn_R_50_FPN_3x.pth
INDICO OUTPUT [[{'label': 'Table', 'left': 126, 'right': 1141, 'top': 716, 'bottom': 1120, 'confidence': 0.9895569682121277, 'image': &lt;_io.BytesIO object at 0x7f569f52e0b0&gt;}]]
Trying out layout parser...
LAYOUT PARSER OUTPUT Layout(_blocks=[TextBlock(block=Rectangle(x_1=126.4218521118164, y_1=716.7941284179688, x_2=1141.1806640625, y_2=1120.6494140625), text=None, id=None, type=Table, parent=None, next=None, score=0.9895569682121277)], page_data={})
</code></pre><h2 id="32-results">3.2 Results</h2>
<p><em>Each model was trained on Lease PNG label set v1 and evaluated on the test split.</em></p>
<figure>
    <img src="img/eval_AP_layout_parser.png"
         alt="Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_layout_parser.png"
         alt="Average Recall (COCOEvaluator)"/> <figcaption>
            <p>Average Recall (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_layout_parser.png"
         alt="Per Class Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<p>In these metrics, the baseline model did slightly better overall, especially in Average Precision by class. It was followed closely by the small model baseline and NewspaperNavigator, with both TableBanks doing the worst. The ported layout-parser models did not realize a significant improvement when detecting the target classes. Their pre-training on detecting newspaper components and detecting tables in academic papers perhaps did not give them an edge in the task of detecting filled and unfilled checkboxes, signatures, redaction, and handwriting.</p>
<h1 id="4-expand-the-label-set">4 EXPAND THE LABEL SET</h1>
<p>Since the baseline model had good evaluation numbers on label set v2, the next step was expanding the label set to handle tougher classes. The new label set (v2) had 9 classes, keeping the 5 classes from v1 and adding 4 new, somewhat visually obvious classes with 200+ counts: “Date”, “Initials”, “Stamp”, and “Strikethrough”.</p>
<p>Since “Stamp” and “Strikethrough” looked like very visually distinguishable classes, any multi-label that included the “Stamp” label and “Strikethrough” label were mapped to the “Stamp” class and “Strikethrough” class, respectively, to make the new label set easier to learn.</p>
<p>The label set v2 Lease-PNG dataset had the following total instance counts:</p>
<pre><code>all_counts: {'Signature': 2333, 'Date': 1299, 'Initials': 3427, 'Handwriting': 2496, 'Redaction': 3101, 'Stamp': 401, 'Filled Checkbox': 899, 'Unfilled Checkbox': 537, 'Strikethrough': 234}
</code></pre><h2 id="41-results">4.1 Results</h2>
<p><em>Note: The performance of the model on v1 and v2 cannot be directly compared because their train and test splits contained different documents. Despite this, some conclusions can still be drawn. Each model was trained on the Lease-PNG dataset and evaluated on the test split.</em></p>
<figure>
    <img src="img/eval_AP_v2.png"
         alt="Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_v2.png"
         alt="Average Recall (COCOEvaluator)"/> <figcaption>
            <p>Average Recall (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_v2.png"
         alt="Per Class Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<p>Overall, the model trained on v2 had a decent per class AP on the expanded v2 label set, with the exception of the ‘Strikethrough’ class.</p>
<h2 id="42-analysis-of-class-confusion-via-visual-inspection">4.2 Analysis of Class Confusion Via Visual Inspection</h2>
<p>Additionally, visual inspection of the predictions showed that the model sometimes struggled with distinguishing between ‘Filled Checkbox’ vs. ‘Unfilled Checkbox’ and ‘Stamp’ vs. ‘Date’.</p>
<p>At first, to get a sense of the error modes of the model predictions, I glanced through the visualized model predictions and noted down interesting and common errors. <em>Following further analysis I came to realize that some of my conclusions were subject to confirmation bias (you see an error early on so you look for that error more than other errors). I have kept this analysis here because (1) it was an important realization to learn, and (2) this analysis led us to future decisions. For the more objective analysis I performed later, see the <a href="#9-qualitative-and-quantitative-analysis-of-object-detection-class-confusion-false-positives-and-false-negatives">qualitative and quantitative analysis</a> section below.</em></p>
<p><strong>Strikethrough</strong></p>
<p>Instances of ‘Strikethrough’ had a fair bit of variation (scribble, line through word, line through paragraph). There were a small number of examples to learn from (there were less than 230 instances of ‘Strikethrough’ in the training set), and some examples were not labeled, so the model&rsquo;s poor performance was understandable.</p>
<p><strong>Filled Checkbox vs. Unfilled Checkbox</strong></p>
<p>In the “Filled Checkbox” vs. “Unfilled Checkbox”, the predictions sometimes missed the checkbox or labeled it incorrectly. There weren&rsquo;t many errors here:</p>
<figure>
    <img src="img/pred_missed_checkboxes.png"
         alt="Lease PNG document image (LCT04791-Lease-0.png) as an example of predictions missing the very small checkboxes at the bottom of the page. The model predicted these correctly on another similar document (LFL61825-Lease_Z-0.png) so maybe the skew on this particular document affected it here."/> <figcaption>
            <p>Lease PNG document image (LCT04791-Lease-0.png) as an example of predictions missing the very small checkboxes at the bottom of the page. The model predicted these correctly on another similar document (LFL61825-Lease_Z-0.png) so maybe the skew on this particular document affected it here.</p>
        </figcaption>
</figure>

<figure>
    <img src="img/pred_checkbox_errors.png"
         alt="More examples of incorrectly predicted checkboxes identified via visual inspection (aside from these, the checkboxes from the Lease_PNGs dataset were generally neat and marked with an even X)."/> <figcaption>
            <p>More examples of incorrectly predicted checkboxes identified via visual inspection (aside from these, the checkboxes from the Lease_PNGs dataset were generally neat and marked with an even X).</p>
        </figcaption>
</figure>

<p><strong>Stamp vs. Date</strong></p>
<p>The ‘Stamp’ vs. ‘Date’ confusion could be attributed to my label aggregation decision. In label set v2, I had mapped all ground truth [‘Date’, ‘Stamp’] multi-labels to the ‘Stamp’ class, and the model sporadically predicted those as “Date” or “Stamp” <em>(in my Confusion Matrix based analysis I found out that this occurred less frequently that I had first assumed)</em>.</p>
<figure>
    <img src="img/pred_stamp-date.png"
         alt="Examples of model predictions of Date (Orange) and Stamp (Brown) from Lease-PNG v2 test split."/> <figcaption>
            <p>Examples of model predictions of Date (Orange) and Stamp (Brown) from Lease-PNG v2 test split.</p>
        </figcaption>
</figure>

<p>Despite the low frequency, the nature of this class confusion still identified a major issue: there is no right way to map two different axes of variation onto one set of single labels without taking the explicit set of all pairs.</p>
<p>In the single label scheme, if a “Date-Stamp” has a mapped ground truth of “Stamp” and it was predicted as “Date” or two separate “Date” and “Stamp” predictions, the Softmax loss function and metrics say that the model is completely wrong.</p>
<h2 id="43-considerations-on-an-alternative-formulation">4.3 Considerations on an Alternative Formulation</h2>
<p>Using a pairing as a single label (e.g. “Date-Stamp”) would give the model the option to label a predicted instance as both “Date” and “Stamp”, but there are some problems with this solution. If the ‘Date-Stamp’ pairing was a label, at ~200 counts, the class would have a low instance count. Furthermore, the model would not be able to use its knowledge of the “Date-Stamp” class to improve its prediction on other &ldquo;Date-&rdquo; or “Stamp-” hybrid classes. At this point, we decided to experiment with a multi-label model formulation to allow the model to learn multiple axes of variation at once and take better advantage of the original multi-label formulation of the ground truth label set.</p>
<p>There is an alternative formulation that might have been better suited to this problem: using a multi-task concept, where there is one loss for style (e.g. ‘Stamp’) and one loss for content type (e.g. ‘Date’). However, the indico platform currently has a multi-label concept, but not a multi-task concept, so implementing a multi-task formulation for this problem would have been harder to support and less generalized.</p>
<h1 id="5-multi-label-object-detection">5 MULTI-LABEL OBJECT DETECTION</h1>
<h2 id="51-detectron-v2-modifications">5.1 Detectron-v2 Modifications</h2>
<p>Making the model handle multi-label required changing (1) the label representation from scalar class index to list of class indexes, and (2) the classification loss from Softmax to binary cross entropy.</p>
<p>The original single-hot encoding was a tensor of length num_classes+1, where only one position can hold a 1 and the last position indicates the background class. The new multi-hot encoding is a tensor of length num_classes, where any number of positions can hold a 1, and a vector of all 0s indicates the background class.</p>
<p>Here is a list of parts of detectron-v2 had to change to be compatible with the multi-label formulation:</p>
<ul>
<li>Create <code>MultilabelDatasetMapper</code> class (extends <code>DatasetMapper</code>)
<ul>
<li>Change <code>annotations_to_instances</code> function to handle gt_classes in multi-label format (instead of a list of int classes, handle as a list of multi-hot encodings)
Create <code>MultilabelFastRCNNOutputs</code> class (extends <code>nn.Module</code>, same interface as <code>:class:FastRCNNOutputLayers</code>.)</li>
<li>Change <code>fast_rcnn_inference_single_image</code> to not remove last column because vector is multi-label and to handle post NMS merging of multiple <code>pred_classes</code> and scores into a list corresponding to Boxes</li>
<li>Change <code>losses</code> to cat multi-label class gt and predictions and to use <code>sigmoid_cross_entropy</code> instead of Softmax classification loss</li>
<li>Change <code>box_reg_loss</code> to keep instances that are labeled foreground (not zero vectors), and handle getting class-wise box regression loss if multiple classes apply OR (better) in <code>box_reg_loss</code> use class-agnostic bounding box regression by setting <code>MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG</code> in the model config file. This option modifies the final box projection layer to output a tensor of length 4, instead of a tensor of length num_classes x 4.</li>
</ul>
</li>
<li>Create <code>MultilabelROIHeads</code> class (extends <code>StandardROIHeads</code>)
<ul>
<li>Change <code>_init_box_head</code> to use <code>ret[&quot;box_predictor&quot;] = MultilabelFastRCNNOutputs(cfg, ret[&quot;box_head&quot;].output_shape)</code></li>
<li>Change <code>_sample_proposals</code> to handle <code>gt_classes</code> in a multi-label format</li>
<li>Change <code>subsample_labels</code> to handle getting indexes of foreground and background when they are in multi-label vector format</li>
</ul>
</li>
<li>Comment out lines that complain when multi-label
<ul>
<li>The config function for <code>build_detection_train_loader</code> and <code>build_detection_test_loader</code> called <code>get_detection_dataset_dicts</code> which had lines that complained when instances had a multi-label format</li>
</ul>
</li>
<li>Evaluation
<ul>
<li>Write a <code>MultilabelEvaluator</code>, which is a <code>COCOEvaluator</code> modified to handle a binary evaluation problem. For the given class, it keeps the annotations of that class.</li>
<li>Use a list of <code>MultilabelEvaluator</code>, one for each class.</li>
</ul>
</li>
</ul>
<h2 id="52-new-label-sets">5.2 New Label Sets</h2>
<p><strong>Multi-label set v1:</strong> simplest v1 Lease-PNG multi-label set, easily compare its performance to the baseline single-label model.</p>
<pre><code>all_counts: {'Handwriting': 6796, 'Redaction': 3101, 'Signature': 2331, 'Filled Checkbox': 899, 'Unfilled Checkbox': 537}
</code></pre><p><strong>Multi-label set v3:</strong> the “Checkbox” and “Unfilled Checkbox” had a drop in AP score for the multi-label model compared to the single-label model. Introduced a separate “Checkbox” class to multi-label mark both “Filled Checkbox” and “Unfilled Checkbox&rdquo; to help the multi-label model detect checkboxes better,</p>
<pre><code>all_counts: {'Handwriting': 6796, 'Redaction': 3101, 'Signature': 2331, 'Filled': 899, 'Checkbox': 1436, 'Unfilled': 537}
</code></pre><p><strong>Multi-label set v4:</strong> removed the redundant “Unfilled Checkbox” class since its attributes overlapped with the “Checkbox” class.</p>
<pre><code>all_counts: {'Handwriting': 6796, 'Signature': 2331, 'Checkbox': 1439, 'Filled': 901, 'Redaction': 3102}
</code></pre><h2 id="53-results">5.3 Results</h2>
<p><em>Note: The performance of the model on label sets v1, v3, and v4 cannot be directly compared because their train and test splits contained different documents. Despite this, some conclusions can still be drawn. Each model was trained on the Lease-PNG dataset and evaluated on the test split.</em></p>
<figure>
    <img src="img/eval_AP50_multi.png"
         alt="Per Class Average Precision 50 for multi-label models (Modified Binary COCOEvaluator). *For single-label v1 model overall AP50 = 0.838"/> <figcaption>
            <p>Per Class Average Precision 50 for multi-label models (Modified Binary COCOEvaluator). *For single-label v1 model overall AP50 = 0.838</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_multi.png"
         alt="Per Class Average Recall for multi-label models (Modified Binary COCOEvalulator) *For single-label v1 model overall (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502"/> <figcaption>
            <p>Per Class Average Recall for multi-label models (Modified Binary COCOEvalulator) *For single-label v1 model overall (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_single_vs_multi.png"
         alt="Per Class Average Precision for single- and multi-label models (Modified Binary COCOEvaluator and COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision for single- and multi-label models (Modified Binary COCOEvaluator and COCOEvaluator)</p>
        </figcaption>
</figure>

<p>Overall, there was not a significant improvement from switching from single-label to multi-label or from using differently aggregated multi-label sets. The multi-label formulation appears to do 5-10 points worse in Unfilled Checkbox AP.</p>
<p>In both the single and multi-label formulation, the bigger size classes (‘Handwriting’, ‘Signature’, ‘Redaction’) had AP numbers that were 10+ points higher than the smaller size classes (‘Filled Checkbox’ and ‘Unfilled Checkbox’). Looking at multi-label AP50, the model seems decent at identifying a checkbox, and worse at telling whether the checkbox was filled or unfilled.</p>
<h1 id="6-better-small-object-detection">6 BETTER SMALL OBJECT DETECTION</h1>
<p>Detecting small objects is a known trouble spot for object-detection models.</p>
<p><strong>Useful Resources</strong></p>
<ul>
<li><a href="https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9">Anchor Boxes — The key to quality object detection</a> (best, quick read)</li>
<li><a href="https://blog.roboflow.com/detect-small-objects/">Tackling the Small Object Problem in Object Detection</a></li>
<li><a href="https://medium.datadriveninvestor.com/small-objects-detection-problem-c5b430996162">Small objects detection problem | by Quantum</a></li>
</ul>
<p>There are two main approaches I decided to examine: (1) increase image resolution so small objects are bigger, and (2) adjust anchor sizes for better small object bounding box regression.</p>
<h2 id="61-visualize-feature-map">6.1 Visualize Feature Map</h2>
<p>In the default DataMapper, the following transform is applied to all the images:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[ResizeShortestEdge(short_edge_length=(<span style="color:#ae81ff">640</span>, <span style="color:#ae81ff">672</span>, <span style="color:#ae81ff">704</span>, <span style="color:#ae81ff">736</span>, <span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">800</span>), max_size=<span style="color:#ae81ff">1333</span>, sample_style=<span style="color:#e6db74">&#39;choice&#39;</span>), RandomFlip()]
</code></pre></div><p>Images are rescaled to have a shortest edge of length 800 (for test) or randomly chosen from (640, 672, 704, 736, 768, 800) (for train). The RandomFlip means the DataMapper randomly chooses some images to get a horizontal flip transform.</p>
<figure>
    <img src="img/feature_maps_c0.png"
         alt="Given LAK07176-Lease-a_Z-0.png as the input image, the FPN &#43; ResNet backbone net creates the following 5 feature maps (visualized channel 0)"/> <figcaption>
            <p>Given LAK07176-Lease-a_Z-0.png as the input image, the FPN + ResNet backbone net creates the following 5 feature maps (visualized channel 0)</p>
        </figcaption>
</figure>

<p>If the original image is 2556x3305 pixels, it is rescaled to be 800x1056.</p>
<ul>
<li>feature map p2 is created with a stride of 4 (200x264)</li>
<li>feature map p3 is created with a stride of 8 (100x132)</li>
<li>feature map p4 is created with a stride of 16 (50x66)</li>
<li>feature map p5 is created with a stride of 32 (25x33)</li>
<li>feature map p6 is created with a stride of 64 (12x16)</li>
</ul>
<figure>
    <img src="img/feature_map_p2_c0.png"
         alt="Close up of the second column of checkboxes on channel 0 of p2, the highest resolution feature map. The smallest cell anchor size (32x32 pixels, so 8x8 pixels on p2) is drawn in red for size comparison."/> <figcaption>
            <p>Close up of the second column of checkboxes on channel 0 of p2, the highest resolution feature map. The smallest cell anchor size (32x32 pixels, so 8x8 pixels on p2) is drawn in red for size comparison.</p>
        </figcaption>
</figure>

<figure>
    <img src="img/feature_map_p2_c235.png"
         alt="Close up of the second column of checkboxes on channel 235 of p2, the highest resolution feature map. The smallest cell anchor size (32x32 pixels, so 8x8 pixels on p2) is drawn in red for size comparison."/> <figcaption>
            <p>Close up of the second column of checkboxes on channel 235 of p2, the highest resolution feature map. The smallest cell anchor size (32x32 pixels, so 8x8 pixels on p2) is drawn in red for size comparison.</p>
        </figcaption>
</figure>

<p>These images show that the smallest anchor size is significantly bigger than the checkbox feature representation, so the model has to learn a large regression change to fit the checkbox. Furthermore, at this resolution, the checkboxes appear as roughly 2x2 cell areas, so there is not enough detail to discriminate between filled and unfilled checkboxes.</p>
<h2 id="62-increase-image-resoluton">6.2 Increase Image Resoluton</h2>
<p>Increasing the image resolution allows more detail to be captured in the feature maps.</p>
<p>I changed the DataMapper image transform to</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[ResizeShortestEdge(short_edge_length=(<span style="color:#ae81ff">1600</span>,), max_size=<span style="color:#ae81ff">2666</span>, sample_style=<span style="color:#e6db74">&#39;choice&#39;</span>)]
</code></pre></div><p>This means the image is rescaled to double the default size (note: for the all documents dataset, I set max_size to 2071 so that images with certain ratios would not cause CUDA out of memory issues). I also disabled the RandomFlip (horizontal flip transform) because it is less applicable for document images.</p>
<figure>
    <img src="img/feature_map_p2_c235_double.png"
         alt="Close up of the second column of checkboxes on channel 235 of p2, the highest resolution feature map. The smallest cell anchor size (32x32 pixels, so 8x8 pixels on p2) is drawn in red for size comparison."/> <figcaption>
            <p>Close up of the second column of checkboxes on channel 235 of p2, the highest resolution feature map. The smallest cell anchor size (32x32 pixels, so 8x8 pixels on p2) is drawn in red for size comparison.</p>
        </figcaption>
</figure>

<p>The checkboxes are represented in higher resolution, so it is slightly easier to discern filled vs. unfilled checkboxes. The cell anchor regression box is also closer to the expected checkbox size, so learning the regression parameters should be simpler.</p>
<p><strong>Room for Improvement:</strong> Instead of just one number (1600,) the data transform could include a range of train-time resize transformations (as the default does). This would allow the model to learn features and proposals at a variety of different scales.</p>
<h2 id="63-add-smaller-anchor-size">6.3 Add Smaller Anchor Size</h2>
<p>Adding a smaller anchor size does not help with feature resolution, but it could improve recall for smaller classes since the regression parameters are simpler and matching proposals to ground truth by intersection over union (IoU) during training should work better.</p>
<p>I added a 16 cell anchor size to the model configuration.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">MODEL.ANCHOR_GENERATOR.SIZES = [[<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>]]
</code></pre></div><figure>
    <img src="img/feature_map_p2_c0_anchor_16.png"
         alt="Close up of the second column of checkboxes on channel 235 of p2, the highest resolution feature map. The smallest cell anchor size (16x16 pixels, so 4x4 pixels on p2) is drawn in red for size comparison."/> <figcaption>
            <p>Close up of the second column of checkboxes on channel 235 of p2, the highest resolution feature map. The smallest cell anchor size (16x16 pixels, so 4x4 pixels on p2) is drawn in red for size comparison.</p>
        </figcaption>
</figure>

<p>On the highest resolution feature map (p2), the smallest cell anchor (size 16) fits a single checkbox much better.</p>
<h2 id="64-results">6.4 Results</h2>
<p><em>Each model was trained on the Lease-PNG dataset label set v4 and evaluated on the test split.</em></p>
<p><strong>Inference Times:</strong> For an image resized by the default DataMapper, inference takes 0.067 sec / img. For an image double the default resize, inference takes 0.172 sec / img, or roughly 7x longer than inference on an image with the default resize transform.</p>
<p><strong>Results on Multi-Label Model</strong></p>
<figure>
    <img src="img/eval_AP50_multi_detect_smaller_objects.png"
         alt="Per Class Average Precision 50 for multi-label models (Modified Binary COCOEvaluator). *For single-label v1 model overall AP50 = 0.838"/> <figcaption>
            <p>Per Class Average Precision 50 for multi-label models (Modified Binary COCOEvaluator). *For single-label v1 model overall AP50 = 0.838</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_multi_detect_smaller_objects.png"
         alt="Per Class Average Recall for multi-label models (Modified Binary COCOEvalulator) *For single-label v1 model overall (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502"/> <figcaption>
            <p>Per Class Average Recall for multi-label models (Modified Binary COCOEvalulator) *For single-label v1 model overall (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_multi_detect_smaller_objects.png"
         alt="Per Class Average Precision for single- and multi-label models (Modified Binary COCOEvaluator and COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision for single- and multi-label models (Modified Binary COCOEvaluator and COCOEvaluator)</p>
        </figcaption>
</figure>

<p>Looking at AP, AP50, and AR metrics for the multi-label models, doubling the image resize resulted in a slight improvement across all classes, especially across “Filled” and “Checkbox” classes. Adding the cell anchor size 16 did not noticeably improve the metrics.</p>
<figure>
    <img src="img/pred_multi_default_vs_doubled.png"
         alt="Predictions on LAK07176-Lease-a_Z-0.png with the default image resize transform (top) compared to the doubled image resize transform (bottom). With the default image resize transform the model missed a checkbox completely, and twice marked an unfilled checkbox as “Filled”. With the doubled image resize transform, the model did not make those errors."/> <figcaption>
            <p>Predictions on LAK07176-Lease-a_Z-0.png with the default image resize transform (top) compared to the doubled image resize transform (bottom). With the default image resize transform the model missed a checkbox completely, and twice marked an unfilled checkbox as “Filled”. With the doubled image resize transform, the model did not make those errors.</p>
        </figcaption>
</figure>

<p><strong>Results on Single-Label Model</strong></p>
<figure>
    <img src="img/eval_AP_detect_smaller_objects.png"
         alt="Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_detect_smaller_objects.png"
         alt="Average Recall (COCOEvaluator)"/> <figcaption>
            <p>Average Recall (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_detect_smaller_objects.png"
         alt="Per Class Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<p>The single-label model was already performing well on the smaller classes. Doubling the image resize transform did not lead to a great improvement. I found this interesting, as the single-label models and multi-label models share the same Backbone network.</p>
<h1 id="7-test-on-out-of-distribution-data">7 TEST ON OUT-OF-DISTRIBUTION DATA</h1>
<p>Taking a model trained on one dataset and testing it on out-of-distribution (OOD) dataset is one way to sense how robust the model is.</p>
<p>The best performing model (in terms of both metrics and inference time) was the baseline single-label model with default transforms. In this evaluation, I took the best performing model trained on Lease PNGs and evaluated its predictions on FCC PNGs dataset.</p>
<h2 id="71-results">7.1 Results</h2>
<figure>
    <img src="img/eval_AP_OOD.png"
         alt="Per Class Average Precision (COCOEvaluator) &amp;ndash; evaluated on the full FCC dataset"/> <figcaption>
            <p>Per Class Average Precision (COCOEvaluator) &ndash; evaluated on the full FCC dataset</p>
        </figcaption>
</figure>

<p>The “Signature” and “Date” classes performed very well on this OOD test. Given this performance, the “Date” class was included in the final label set (v5).</p>
<h1 id="8-train-final-all_document-model">8 TRAIN FINAL ALL_DOCUMENT MODEL</h1>
<p>The best performing model was the single-label baseline model (pre-trained on ImageNet and COCO) with default image resize transformations and cell anchors. The final model was trained on the all_doc dataset with v5 label set for 50,000 iterations.</p>
<h2 id="81-results">8.1 Results</h2>
<figure>
    <img src="img/eval_AP_all_docs.png"
         alt="Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_AR_all_docs.png"
         alt="Average Recall (COCOEvaluator)"/> <figcaption>
            <p>Average Recall (COCOEvaluator)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/eval_APclass_all_docs.png"
         alt="Per Class Average Precision (COCOEvaluator)"/> <figcaption>
            <p>Per Class Average Precision (COCOEvaluator)</p>
        </figcaption>
</figure>

<p>The evaluation metrics from the final model (last row) have been placed on the same table as the model baseline v1, model baseline v2, and the runs tested on OOD FCC PNGs data, for comparison. The final model performs unusually badly (numbers &lt;10) on most classes, especially Filled and Unfilled Checkbox. It also seems to do somewhat poorly (numbers ~20) on Signature, Date, and Handwriting Detection. The only class it retains good performance on is Redaction.</p>
<h2 id="82-train-validation-loss-curve">8.2 Train-Validation Loss Curve</h2>
<p>Given a poorly performing model, the first step in analysis is identifying whether the model is over-fitting or under-fitting. This can be accomplished by plotting the train and validation loss over model iterations.</p>
<p>I had run a custom train hook to evaluate the model on the test split every 2000 steps and log the validation loss (the hook ran 25 times, performed inference on ~500 images each time, overall adding ~2 hours to the total training time). The train and validation loss both steadily decreased and leveled off over the course of training.</p>
<figure>
    <img src="img/train_val_loss.png"
         alt="Total Loss vs. Validation Loss from training all_doc model on 50,000 iterations"/> <figcaption>
            <p>Total Loss vs. Validation Loss from training all_doc model on 50,000 iterations</p>
        </figcaption>
</figure>

<p>Resources</p>
<ul>
<li>How to use TensorBoard with PyTorch — PyTorch Tutorials 1.9.0+cu102 documentation</li>
<li>Training on Detectron2 with a Validation set, and plot loss on it to avoid overfitting</li>
</ul>
<h1 id="9-qualitative-and-quantitative-analysis-of-object-detection-class-confusion-false-positives-and-false-negatives">9 QUALITATIVE AND QUANTITATIVE ANALYSIS OF OBJECT DETECTION CLASS CONFUSION, FALSE POSITIVES, AND FALSE NEGATIVES</h1>
<p>It is very useful to analyze a model&rsquo;s error modes, especially class confusion, false positives, and false negatives. I wrote a script to create a object-detection confusion matrix to identify class confusion and unpaired predictions and ground truths, and visualizing high and low confidence predictions by prediction label and ground truth label.</p>
<p>I’ll focus this analysis on the test split predictions from the single-label models trained on</p>
<ul>
<li>Lease-PNG label set v1 with default transform</li>
<li>Lease-PNG label set v1 with doubled image resize transform</li>
<li>Lease-PNG label set v2</li>
<li>All_Docs label set v5</li>
</ul>
<p>To create the object-detection confusion matrices, I first created a pairwise IoU matrix between all the predictions and the ground truth bounding boxes for each image. I discarded pairings with an IoU less than the threshold (0.5) and then matched each prediction with the ground truth it has the most IoU with. In this way, predictions can be matched with 1 or 0 ground truths, and ground truths can be matched with any number of predictions. The confusion matrix indicates whether the prediction’s label agrees with the closest ground truth’s label or if there is class confusion. If a prediction has no closest ground truth (no IoU &gt; threshold) then it is counted in the unmatched column. If a ground truth has no matches with predictions, then it is counted in the unmatched row.</p>
<p>Then, to get a better visual sense of error modes, I made panels that visualized the predictions in terms of predicted label vs. ground truth label and ordered instances by model confidence. If the pairing had less than or equal to 100 predictions, I displayed all of them. If it had between 100 and 200 predictions, I displayed an evenly indexed range of predictions based on confidence value order. If it had more than 200 predictions, I displayed the top 100 high confidence and bottom 100 low confidence predictions. After examining these panels, I marked the potential real cases of class confusion in red text on the confusion matrices tables.</p>
<h2 id="91-performance-on-lease-png-label-set-v1">9.1 Performance on Lease-PNG label set v1</h2>
<figure>
    <img src="img/confusion_v1.png"
         alt="confusion matrix for single-label model baseline trained on Lease-PNG label set v1 with default transform"/> <figcaption>
            <p>confusion matrix for single-label model baseline trained on Lease-PNG label set v1 with default transform</p>
        </figcaption>
</figure>

<figure>
    <img src="img/confusion_v1_double.png"
         alt="confusion matrix for single-label model baseline trained on Lease-PNG label set v1 with doubled image resize transform"/> <figcaption>
            <p>confusion matrix for single-label model baseline trained on Lease-PNG label set v1 with doubled image resize transform</p>
        </figcaption>
</figure>

<p>These confusion matrices were created from model predictions on the Lease PNG label set v1’s test set. The confusion matrices for the model baseline with default transform images and the model baseline with double image resize transform predictions both look pretty similar. Both have strong agreement between prediction labels and ground truth labels (high counts on the diagonal), and do a decent job detecting Filled Checkboxes and Unfilled Checkboxes (a small portion of checkbox ground truths are unpaired, the model with doubled resized images does slightly better here). In terms of undesirable behavior, both have many unmatched predictions that did not overlap with any ground truths (especially for the Handwriting class), a scattering of predictions with class confusion, and a small number of unmatched ground truths that did not overlap with a prediction (especially for the Handwriting class).</p>
<p><strong>Unmatched Predictions Visualized</strong></p>
<figure>
    <img src="img/confusion_panel_default_unmatched_prediction.png"
         alt="Unmatched predictions from model baseline with default transform: Predicted-Filled Checkbox (top-left), Predicted-Unfilled Checkbox (middle-left), Predicted-Redaction (bottom-left), Predicted-Signature (top-right), Predicted-Handwriting (bottom-right)"/> <figcaption>
            <p>Unmatched predictions from model baseline with default transform: Predicted-Filled Checkbox (top-left), Predicted-Unfilled Checkbox (middle-left), Predicted-Redaction (bottom-left), Predicted-Signature (top-right), Predicted-Handwriting (bottom-right)</p>
        </figcaption>
</figure>

<p>For the model baseline with default transform’s predictions, the vast majority of predictions not matching a ground truth seem to be correctly predicted, but missing ground truth labels. For the unmatched “Redaction” class, there are maybe 6 instances where white text on a black background has been predicted as Redaction. Overall, no real problems. Three of the unmatched “Filled Checkboxes” may have been unmatched due to the predicted bounding box covering the area of two checkboxes (and thus likely exceeding the 0.5 IoU threshold).</p>
<figure>
    <img src="img/confusion_panel_doubled_unmatched_prediction.png"
         alt="Unmatched predictions from model baseline with doubled image resize transform: Predicted-Filled Checkbox (top-left), Predicted-Unfilled Checkbox (middle-left), Predicted-Redaction (bottom-left), Predicted-Signature (top-right), Predicted-Handwriting (bottom-right)"/> <figcaption>
            <p>Unmatched predictions from model baseline with doubled image resize transform: Predicted-Filled Checkbox (top-left), Predicted-Unfilled Checkbox (middle-left), Predicted-Redaction (bottom-left), Predicted-Signature (top-right), Predicted-Handwriting (bottom-right)</p>
        </figcaption>
</figure>

<p>Like the model with default transform, the vast majority of predictions not matching a ground truth seem to be correctly predicted, but missing ground truth labels. The issue where three of the predicted “Filled Checkboxes” had too large bounding box areas has gone away, indicating that the bounding box regression was likely more successful on the larger images.</p>
<p><strong>Class Confusion Visualized</strong></p>
<figure>
    <img src="img/confusion_panel_baseline_class_confusion_false.png"
         alt="Incorrectly marked class confusion from model baseline with default transform: Predicted-Handwriting:GT-Redaction (top-left), Predicted-Unfilled Checkbox:GT-Filled Checkbox (middle-left), Predicted-Handwriting:GT-Unfilled Checkbox (bottom-left), Predicted-Redaction:GT-Handwriting (top-right), Predicted-Signature:GT-Redaction (bottom-right)."/> <figcaption>
            <p>Incorrectly marked class confusion from model baseline with default transform: Predicted-Handwriting:GT-Redaction (top-left), Predicted-Unfilled Checkbox:GT-Filled Checkbox (middle-left), Predicted-Handwriting:GT-Unfilled Checkbox (bottom-left), Predicted-Redaction:GT-Handwriting (top-right), Predicted-Signature:GT-Redaction (bottom-right).</p>
        </figcaption>
</figure>

<p>In terms of class confusion, there is very little. For all the predictions in pairings of Predicted-Handwriting:GT-Redaction (2), Predicted-Unfilled Checkbox:GT-Filled-Checkbox (3), Predicted-Handwriting:GT-Unfilled Checkbox (1), Predicted-Redaction:GT-Handwriting (18), and Predicted-Signature:GT-Redaction (2), the ground truth label is wrong and the predicted label is right.</p>
<figure>
    <img src="img/confusion_panel_baseline_class_confusion_true.png"
         alt="Real and potentially real class confusion form model baseline with default transform. From top to bottom: Predicted-Handwriting:GT-Filled Checkbox, Predicted-Filled Checkbox:GT-Unfilled Checkbox, Predicted-Signature:GT-Handwriting, Predicted-Handwriting:GT-Signature"/> <figcaption>
            <p>Real and potentially real class confusion form model baseline with default transform. From top to bottom: Predicted-Handwriting:GT-Filled Checkbox, Predicted-Filled Checkbox:GT-Unfilled Checkbox, Predicted-Signature:GT-Handwriting, Predicted-Handwriting:GT-Signature</p>
        </figcaption>
</figure>

<p>Aside from a single Filled Checkbox predicted as Handwriting and 7 Unfilled checkboxes predicted as Filled, there is no obvious class confusion. The class confusion between Handwriting and Signature class may exist. Without additional context clues, all 7+24 instances of this confusion look like they could be either Signatures or messily/cursive Handwritten names.</p>
<h2 id="92-performance-on-lease-png-label-set-v2">9.2 Performance on Lease-PNG label set v2</h2>
<figure>
    <img src="img/confusion_v2.png"
         alt="confusion matrix for single-label model baseline trained on Lease-PNG label set v2 with default transform"/> <figcaption>
            <p>confusion matrix for single-label model baseline trained on Lease-PNG label set v2 with default transform</p>
        </figcaption>
</figure>

<p>This confusion matrix reveals some of the reasons behind poor AP performance for certain classes, namely that Stamp and Unfilled Checkbox had ~40 predictions to be evaluated on, and Strikethrough did not even detect most of the ground truth strikethrough instances.</p>
<p>In terms of desired behavior, the model did very well on detecting the other classes, especially the small checkbox classes (high counts on the diagonals and very few instances of class confusion or unmatched ground truths).</p>
<h2 id="93-performance-on-all_docs-label-set-v5-final-model">9.3 Performance on All_Docs label set v5 (Final Model)</h2>
<figure>
    <img src="img/confusion_all_docs.png"
         alt="confusion matrix for single-label model baseline trained on All_Docs label set v5 with default transform"/> <figcaption>
            <p>confusion matrix for single-label model baseline trained on All_Docs label set v5 with default transform</p>
        </figcaption>
</figure>

<p>The final document model, trained on the all documents dataset using label set v5 for 50k iterations, had a pretty bad looking confusion matrix. In particular, almost all the Filled Checkbox and Unfilled Checkbox failed to be detected.</p>
<figure>
    <img src="img/pred_all_doc.png"
         alt="All Doc document image (0003f6ad-e10a-9b4a-3d73-9d387656d39c-1.png) as an example of predictions missing the clean checkboxes in the center row of the page."/> <figcaption>
            <p>All Doc document image (0003f6ad-e10a-9b4a-3d73-9d387656d39c-1.png) as an example of predictions missing the clean checkboxes in the center row of the page.</p>
        </figcaption>
</figure>

<p>The confusion matrix also indicates several unmatched Handwriting predictions, some unmatched Filled Checkboxes and Unfilled Checkboxes, and class confusion between Handwriting and Date, Unfilled Checkbox and Date, and Handwriting and Filled Checkbox.</p>
<p><strong>Unmatched Predictions Visualized</strong></p>
<figure>
    <img src="img/confusion_panel_all_docs_unmatched_predictions_checkboxes.png"
         alt="Unmatched predictions for Unfilled Checkbox (top) and Filled Checkbox (bottom)"/> <figcaption>
            <p>Unmatched predictions for Unfilled Checkbox (top) and Filled Checkbox (bottom)</p>
        </figcaption>
</figure>

<p>The unmatched predictions for the checkbox classes seem to be picking up on circular letters and semi-circle shapes.</p>
<figure>
    <img src="img/confusion_panel_all_docs_unmatched_predictions_date_handwriting.png"
         alt="Unmatched predictions for Date (top), high confidence Handwriting (top-left) and low confidence Handwriting (bottom-right)"/> <figcaption>
            <p>Unmatched predictions for Date (top), high confidence Handwriting (top-left) and low confidence Handwriting (bottom-right)</p>
        </figcaption>
</figure>

<p>The unmatched predictions for Date and Handwriting all seem to be picking up on numbers. Date in particular has lots of instances of page numbers, as well as text on a messy grey background. The Handwriting predictions seem to pick up on standalone numbers, as well as X marks and stamped or unusual font text.</p>
<figure>
    <img src="img/confusion_panel_all_docs_unmatched_predictions_redaction_signature.png"
         alt="Unmatched predictions for Redaction (left) and Signature (right)"/> <figcaption>
            <p>Unmatched predictions for Redaction (left) and Signature (right)</p>
        </figcaption>
</figure>

<p>The unmatched predictions for Redaction and Signature seem to be pretty correct. The model might be predicting redaction in areas with dark backgrounds and not real redaction, so a closer look at the predictions in the context of their document may be needed for validation.</p>
<p><strong>Class Confusion Visualized</strong></p>
<figure>
    <img src="img/confusion_panel_all_docs_date_confusion.png"
         alt="Class confusion when GT-Date is predicted as Handwriting (top) and Signature (bottom)"/> <figcaption>
            <p>Class confusion when GT-Date is predicted as Handwriting (top) and Signature (bottom)</p>
        </figcaption>
</figure>

<p>The biggest class confusion is when visually obvious Date instances are predicted as Handwriting or Signature.</p>
<figure>
    <img src="img/confusion_panel_all_docs_other_class_confusion.png"
         alt="Class confusion when GT-Handwriting predicted as Date (top), when GT-Filled Checkbox predicted as Handwriting (middle). Single instance when GT-Unfilled Checkbox was predicted as Redaction (bottom)."/> <figcaption>
            <p>Class confusion when GT-Handwriting predicted as Date (top), when GT-Filled Checkbox predicted as Handwriting (middle). Single instance when GT-Unfilled Checkbox was predicted as Redaction (bottom).</p>
        </figcaption>
</figure>

<p>There is also some consistent class confusion when slanted initials in the Handwriting class are predicted as Date, or X-shaped or check-shaped marks in Filled Checkboxes are predicted as Handwriting.</p>
<p><strong>Unmatched Ground Truths Visualized</strong></p>
<figure>
    <img src="img/confusion_panel_all_docs_missed_GT.png"
         alt="Unmatched ground truths: GT-Unfilled Checkbox (top-left), GT-Redaction (bottom-left), GT-Filled Checkbox (top-right), GT-Date (bottom-right)"/> <figcaption>
            <p>Unmatched ground truths: GT-Unfilled Checkbox (top-left), GT-Redaction (bottom-left), GT-Filled Checkbox (top-right), GT-Date (bottom-right)</p>
        </figcaption>
</figure>

<p>There is some variance and messiness in the missed ground truths, but not overwhelmingly so. They might have been detected at a confidence lower than the 0.5 threshold? It is interesting that many of these checkboxes come from Lease PNG documents that the previous models could predict fine on.</p>
<h1 id="10-future-work">10 FUTURE WORK</h1>
<h2 id="101-analysis-of-prediction-bounding-box-overlap">10.1 Analysis of Prediction Bounding Box Overlap</h2>
<p>Work could be done to measure and limit overlapping bounding boxes. The Faster-R-CNN model seems to permit overlapping bounding boxes in two cases:</p>
<ol>
<li>Instances of the same class if the instances have less than 0.5 overlap (this evades the ROIHead per-class non-maximal suppression threshold)</li>
<li>Instances of different classes on the same area (this might be evading the Region Proposal Network non-maximal suppression threshold)</li>
</ol>
<figure>
    <img src="img/pred_overlap_same_class.png"
         alt="Example of Overlapping Signature Predictions (same class, less than 0.5 overlap)"/> <figcaption>
            <p>Example of Overlapping Signature Predictions (same class, less than 0.5 overlap)</p>
        </figcaption>
</figure>

<figure>
    <img src="img/pred_overlap_different_class.png"
         alt="Example of Overlapping Handwriting and Signature Predictions (different classes, same area)"/> <figcaption>
            <p>Example of Overlapping Handwriting and Signature Predictions (different classes, same area)</p>
        </figcaption>
</figure>

<h2 id="102-reduce-false-positives-with-explicit-labeling">10.2 Reduce False Positives with Explicit Labeling</h2>
<p>As seen in the all_doc model’s unmatched predictions, there are some recurring false positives, such as page numbers, numbers in general, dark backgrounds, and curvy letters or X marks in unusual fonts. One may be able to teach the model to filter out false positives by explicitly labeling those as another class.</p>
<h2 id="103-reduce-inference-time-and-memory-usage">10.3 Reduce Inference Time and Memory Usage</h2>
<p>The default single-label Faster R-CNN model is rather slow and consumes a lot of memory. It takes ~5 minutes to run inference on ~500 documents. Due to its memory requirements, training it on the dev cluster failed a couple of times.</p>
<p>One potential solution to the memory issue is gradient check-pointing, which is a method used for reducing the memory footprint when training deep neural networks, at the cost of having a small increase in computation time.</p>
<h2 id="104-find-out-why-all_docs-model-performed-poorly-on-checkbox-detection">10.4 Find Out Why All_Docs Model Performed Poorly on Checkbox Detection</h2>
<p>The final all_docs model trained on all the documents struggled to detect Filled Checkboxes and Unfilled Checkboxes, even on documents from the Lease PNG dataset.</p>
<h1 id="11-appendix-faster-rcnn-implementation-details">11 APPENDIX: FASTER-RCNN IMPLEMENTATION DETAILS</h1>
<p>A large portion of my internship involved understanding how Faster-RCNN worked. In hopes of helping future me (and others) with this task in the future, I have placed my clean and organized notes here.</p>
<h2 id="datamapper">DataMapper</h2>
<p>The DataMapper applies transformations to input images. By default, it rescales the images so they are smaller (so shortest edge is a random choice of (640, 672, 704, 736, 768, 800)), and flips images horizontally.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">[ResizeShortestEdge(short_edge_length=(<span style="color:#ae81ff">640</span>, <span style="color:#ae81ff">672</span>, <span style="color:#ae81ff">704</span>, <span style="color:#ae81ff">736</span>, <span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">800</span>), max_size=<span style="color:#ae81ff">1333</span>, sample_style=<span style="color:#e6db74">&#39;choice&#39;</span>), RandomFlip()]
</code></pre></div><h2 id="backbone-network">Backbone Network</h2>
<p>The backbone network receives a transformed input image from the Data Mapper and creates feature maps at various scales by using a Feature Pyramid Network (FPN) and a ResNet. By default, it creates feature maps using 5 strides (S = 4, 8, 16, 32, and 64) called p2, p3, p4, p5, and p6. Each feature map has 256 channels.</p>
<h3 id="details-on-feature-pyramid-network">Details on Feature Pyramid Network</h3>
<p>The Feature Pyramid Network is an accurate and fast feature extractor that replaces the default feature extractor of Faster R-CNN. It is composed of a bottom-up pathway (ResNet convolution network for feature extraction) and a top-down pathway (lateral connections for merging high-level semantic features information into lower feature maps to create higher resolution layers)</p>
<figure>
    <img src="img/FPN.png"
         alt="Feature Pyramid Network top-down and bottom-up pathway. (Source: Understanding Feature Pyramid Networks for object detection (FPN))"/> <figcaption>
            <p>Feature Pyramid Network top-down and bottom-up pathway. (Source: <a href="https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c">Understanding Feature Pyramid Networks for object detection (FPN)</a>)</p>
        </figcaption>
</figure>

<h2 id="region-proposal-network-rpn">Region Proposal Network (RPN)</h2>
<p>The RPN associates the backbone network’s feature maps to ground-truth box locations, uses a binary classifier to determine “objectness score” over the feature maps, and outputs box proposals of regions that probably contain an object. This is where anchors, ground truth locations, and the “objectness” loss functions come into play.</p>
<h3 id="rpn-head">RPN Head</h3>
<p>The RPN uses a convolutional neural network (RPN Head) that takes feature maps and ground truth box locations as its inputs. Then it outputs <code>pred_objectness_logits</code> and <code>pred_anchor_deltas</code>.</p>
<p>For each level of feature map (p2, p3, p4, p5, p6) the RPN Head calculates:</p>
<ol>
<li><code>pred_objectness_logits</code> (B, 3 ch, Hi, Wi): probability map of object existence</li>
<li><code>pred_anchor_deltas</code> (B, 3×4 ch, Hi, Wi): box shape relative to anchors</li>
</ol>
<p>B stands for batch size, Hi and Wi are height and width of the feature map, and the 3 channels correspond to the three potential classes (“foreground”, “background”, and “ignore”).</p>
<p>During training, a loss function is used to determine how close RPN Head objectness predictions are to the ground truth boxes. To compare the <code>pred_objectness_logits</code> map and <code>pred_anchor_deltas</code> map to the ground truth boxes, the ground truth boxes have to be mapped to the feature maps.</p>
<h4 id="map-ground-truths-to-feature-maps-with-cell-anchors-during-training">Map Ground Truths to Feature Maps With Cell Anchors (during training)</h4>
<p>The cell anchor generation step creates <code>objectness_logits</code> (ground truth objectness map) and <code>anchor_deltas</code> (ground truth anchor deltas map) with values at each grid point of the feature map.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># MODEL.ANCHOR_GENERATOR.SIZES = [[32], [64], [128], [256], [512]]</span>
<span style="color:#75715e"># detectron-v2 applied all the cell anchors to all the feature maps </span>
MODEL.ANCHOR_GENERATOR.SIZES = [[<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>]]
MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>]]
</code></pre></div><p>The five elements of the <code>ANCHOR_GENERATOR.SIZES</code> list correspond to five levels of feature maps (P2 to P6). If <code>len(ANCHOR_GENERATOR.SIZES) == 1</code>, then the size <code>ANCHOR_GENERATOR.SIZES[0]</code> cell anchors are applied to each feature map.</p>
<p>Each cell anchor is applied at each grid point of the feature map at each aspect ratio. For example P2 (stride=4) has one cell anchor whose size is 32, so its anchors cover 4x16 cells, 8x8 cells, and 16x4 cells of the p2 feature map, which translates to regions of 22.6x45.2 pixels, 32x32 pixels, and 45.2x22.6 pixels on the transformed input image).</p>
<p>Once the anchors are generated, <code>objectness_logits</code> can be calculated. First, the intersections between all the generated anchors and all the ground truth boxes are calculated in a Intersection-over-Union (IoU) Matrix. Generated anchors with intersection greater than 0.7 are labeled foreground (“1”), less than 0.3 are labeled background (“0”), and in between (0.3 &lt; x &lt; 0.7) are labeled ignored (“-1”).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">MODEL.RPN.IOU_THRESHOLDS = [<span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.7</span>]
MODEL.RPN.IOU_LABELS = [<span style="color:#ae81ff">0</span>, -<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]
</code></pre></div><p>Then <code>anchor_deltas</code> are calculated as the regression parameters (Δx, Δy, Δw, and Δh) between each foreground box and its closest ground truth box.</p>
<h4 id="loss-function-during-training">Loss Function (during training)</h4>
<p>Since the majority of generated anchors are going to be of the “background” class, the labels are re-sampled to fix the imbalance and make it easier to learn foreground classes. Then two loss function are applied:</p>
<ol>
<li>Objectness Loss: Binary cross entropy loss comparing <code>pred_objectness_logits</code> to foreground <code>objectness_logits</code></li>
<li>Localization Loss: L1 loss comparing <code>pred_anchor_deltas</code> to foreground and background <code>anchor_deltas</code></li>
</ol>
<h4 id="box-proposal-selection">Box Proposal Selection</h4>
<p>The RPN applies the <code>pred_anchor_deltas</code>, sorts the predicted boxes by <code>pred_objectness_logits</code>, chooses the top 2,000 boxes from each feature level, applies non-maximum suppression at each level independently, and keeps the surviving 1,000 top-scored region proposal boxes.</p>
<h2 id="roi-head">ROI Head</h2>
<p>The ROI Head is a multi-class classifier that receives the backbone network’s feature maps (p2, p3, p4, and p5 &ndash; p6 is not used), the RPN’s box proposals (1,000 top scoring proposal boxes, their objectness_logits are not used), and the ground truth boxes.</p>
<h3 id="re-sampling-and-matching-during-training">Re-Sampling and Matching (during training)</h3>
<p>Ground truth boxes are added to the 1,000 RPN boxes. Then the IoU matrix is calculated between the box proposals and the ground truth (with proposals labeled as foreground if they have greater than 0.5 IoU and background otherwise; the added ground truths match themselves perfectly). Finally, the group is re-sampled to balance the proportion of foreground and background proposal boxes.</p>
<h3 id="cropping">Cropping</h3>
<p>The ROI pooling process uses the coordinates of the proposal boxes to crop the corresponding rectangular regions of the feature maps. It uses a <a href="https://github.com/facebookresearch/detectron2/blob/4fa6db0f98268b8d47b5e2746d34b59cf8e033d7/detectron2/modeling/poolers.py#L40-L43">level assignment rule</a> to determine which feature map the region of interest (ROI) should be cropped from (ex: the small boxes should crop from p2, big boxes from p5).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Eqn.(1) in FPN paper</span>
    level_assignments <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>floor(
        canonical_level <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>log2(box_sizes <span style="color:#f92672">/</span> canonical_box_size <span style="color:#f92672">+</span> eps)
    )
</code></pre></div><p>Here eps=4, canonical_box_size=224. So if the proposal box was 224x224, it would be assigned to p4.</p>
<p>In order to accurately crop the ROI by the proposal boxes which have floating-point coordinates, a method called ROIAlign has been proposed in the Mask R-CNN paper. In Detectron 2, the default pooling method is called ROIAlignV2, which is the slightly modified version of ROIAlign.</p>
<h3 id="box-head">Box Head</h3>
<p>The Box Head (FastRCNNConvFCHead) classifies the object within the region of interest and adjusts the box&rsquo;s position and shape. It consists of linear fully-connected (FC) layers and two final box_predictor layers that project the output into a scores tensor (B, 80+1) and a prediction deltas tensor (B, 80x4). In the original ImageNet dataset, there were 80 classes. The “+1” final position indicates the background class.</p>
<h4 id="loss-calculation">Loss Calculation</h4>
<p>Two loss functions are applied:</p>
<ol>
<li>Classification Loss: Softmax cross entropy loss comparing prediction scores to ground truth class indexes.</li>
<li>Localization Loss: L1 loss comparing <code>pred_proposal_deltas</code> to foreground <code>gt_proposal_deltas</code></li>
</ol>
<h4 id="inference">Inference</h4>
<p>Just like in the RPN, to get the final output, the ROIHead applies the prediction deltas to get the final box coordinates, filters out low-scoring boxes, uses non-maximum suppression, and returns the top-k results.</p>

</article>


    </main>
    <footer class="site-footer">
        &copy 2021 by Gati Aher &#183;
        Powered by <a class="selective-underline" href="https://gohugo.io/">Hugo</a> &#183;
        View on <a class="selective-underline" href="https://github.com/GatiAher/gatiaher-hugo">GitHub</a>
</footer>
    
<script src="/js/feather.min.js"></script>
<script>
    feather.replace();
</script>
</body>

</html>